{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8ae8767f-9851-4a7e-ba43-029c7d4f87f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import requests\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f812b5f-2705-4542-8ca7-a2c23fc744f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('web_scraping.html', 'r') as html_file:\n",
    "#     content = html_file.read()\n",
    "#     # print(content)\n",
    "#     soup = BeautifulSoup(content, 'lxml')\n",
    "#     # print(soup.prettify())\n",
    "#     courses_html_tags = soup.find_all('h5')\n",
    "#     for course in courses_html_tags:\n",
    "#         print(course.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "638154f4-a48a-4152-a670-804735913dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('web_scraping.html', 'r') as html_file:\n",
    "#     content = html_file.read()\n",
    "#     soup = BeautifulSoup(content, 'lxml')\n",
    "#     course_cards = soup.find_all('div', class_ = 'card')\n",
    "#     for course in course_cards:\n",
    "#         # print(course.h5.text)\n",
    "#         course_name = course.h5.text\n",
    "#         course_price = course.a.text.split()[-1]\n",
    "#         print(f'{course_name} costs {course_price}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "85646e53-cd95-4936-9682-fb713aa30270",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "url = 'https://www.timesjobs.com/candidate/job-search.html?searchType=personalizedSearch&from=submit&txtKeywords=python&txtLocation='\n",
    "html_text =  requests.get(url).text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "38717d83-6556-4f7d-ac3e-c9eb7301f52b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def find_jobs(html_text):\n",
    "    print('Enter skills you are familiar with, and seperate them by comma')\n",
    "    familiar_skills = input('>>>')\n",
    "    print(f'filtering skills: {familiar_skills}....')\n",
    "    skill = familiar_skills.split(', ')\n",
    "    soup = BeautifulSoup(html_text, 'lxml')\n",
    "    jobs = soup.find_all('li', class_ = 'clearfix job-bx wht-shd-bx')\n",
    "    for job in jobs:\n",
    "        date_posted = soup.find('span', class_ = 'sim-posted').text.replace(' ', '')\n",
    "        if 'few' in date_posted: \n",
    "            company_name = job.find('h3', class_ = 'joblist-comp-name').text.replace(' ', '')\n",
    "            job_desc = job.select_one('.list-job-dtl li label').next_sibling\n",
    "            more_info = job.header.h2.a['href']\n",
    "            skills = job.find('span', class_ = 'srp-skills').text.replace(' ', '')\n",
    "            skills_ = skills.strip().split(',')\n",
    "            jobs_ = {}\n",
    "            if all(elem in skills_  for elem in skill) == True:\n",
    "                print(f'Company Name: {company_name.strip()}')\n",
    "                print(f'{job_desc}')\n",
    "                print(f'Required Skills: {skills.strip()}')\n",
    "                print(f'More Info: {more_info.strip()} \\n')\n",
    "    print('Filtering done.')\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "64d115a3-12b1-4590-99f0-c82045a88482",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter skills you are familiar with, and seperate them by comma\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      ">>> return\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filtering skills: return....\n",
      "Filtering done.\n"
     ]
    }
   ],
   "source": [
    "find_jobs(html_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "06c8b993-d6d4-413f-ab25-a6e4f9124a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if __name__ = '__main__':\n",
    "#     while True:\n",
    "#         find_jobs()\n",
    "#         time_wait = 10\n",
    "#         print(f'Waiting for {time_wait} minutes')\n",
    "#         time.sleep(time_wait * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da68838-e8d4-4bcb-93a7-4529f4a8008c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For getting next page\n",
    "\n",
    "# def getdata(url):\n",
    "#     r = requests.get(url)\n",
    "#     soup = BeautifulSoup(r.text, 'lxml')\n",
    "#     return soup\n",
    "\n",
    "# def getnextpage(soup):\n",
    "#     page = soup.find('ul', {'class' : 'a-pagination'}) \n",
    "#     if not page.find('li', {'class' : 'a-disabled a-last'}): # last pagination link\n",
    "#         url = 'http://www.amazon.co.uk/' + str(page.find('li', {'class' : 'a-last'}).find(a)['href'])\n",
    "#         return url\n",
    "#     else:\n",
    "#         return\n",
    "    \n",
    "# while True:\n",
    "#     soup = getdata(url)\n",
    "#     url = getnextpage(soup)\n",
    "#     if not url:\n",
    "#         break\n",
    "#     print(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9427e41b-19eb-426b-a106-c03577b0c3c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering done.\n"
     ]
    }
   ],
   "source": [
    "# soup = BeautifulSoup(html_text, 'lxml')\n",
    "# company_name_ = []\n",
    "# skills_ = []\n",
    "# more_info_ = []\n",
    "# jobs = soup.find_all('li', class_ = 'clearfix job-bx wht-shd-bx')\n",
    "# for job in jobs:\n",
    "#     date_posted = soup.find('span', class_ = 'sim-posted').text.replace(' ', '')\n",
    "#     if 'few' in date_posted: \n",
    "#         company_name = job.find('h3', class_ = 'joblist-comp-name').text.replace(' ', '')\n",
    "#         more_info = job.header.h2.a['href']\n",
    "#         skills = job.find('span', class_ = 'srp-skills').text.replace(' ', '')\n",
    "#         company_name_.append(company_name.strip())\n",
    "#         skills_.append(skills.strip())\n",
    "#         more_info_.append(more_info.strip())\n",
    "# print('Filtering done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d24816c5-0cfb-4b77-b782-65bb1792d62d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.DataFrame({'Company Name' : company_name_, 'Skills' : skills_, 'more_info' : more_info_}) \n",
    "# df.to_csv('Jobs.csv', index = False, encoding = 'utf-8')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
